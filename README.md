 In this project I investigated and compared how effective different methods of finetuning were for finetuning the ResNet-18 model for classification of three facial expressions in the FER-2013 dataset on facial expression. The methods that I analyzed were full finetuning, partial finetuning (wherein I froze the first half of the model and only trained the second half), and Low Rank Adaptation (LoRA). Unsurprisingly, the full finetuning performed the best although by only a small margin and I was surprised at how quickly these three methods converged (granted, ResNet is quite a small model).

However, this project, for me, was really a learning experience more than anything. This project exposed me to new concepts in linear algebra for the LoRA method, using hugging-face for loading pretrained models, different method of data augmentation for creating a more generalizable model (i.e. random transforms on images), and much more.
